{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1-Assignment\n",
    "\n",
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the assignment for Lab 1. \n",
    "\n",
    "**Points**: each exercise is prefixed with the number of points you can obtain for the exercise. However, these points are just an indication of what parts we value more. The assignment itself is assessed as PASS/NO-PASS. In general, we value a critical analysis of running code more than just showing that you can create or run the code. So if you succesfully carried out the instructed commands in a notebook you are not done yet. We want you to analyse, understand what the code is doing with language and text. Be critical, think about how to explain what you observe and write this down in the notebook after running the code. It will be stated clearly in the assignment when we expect this from you.\n",
    "\n",
    "You can make the assignment as a group but make sure that you understand and can carry out the coding yourself as well. You need these skills for your final assignment that is graded. Feedback will be given at the group level.\n",
    "\n",
    "We assume you have worked through the following notebooks:\n",
    "* **Lab1.1-introduction**\n",
    "* **Lab1.2-introduction-to-NLTK**\n",
    "* **Lab1.3-introduction-to-spaCy** \n",
    "\n",
    "In this assignment, you will process an English text (**Lab1-apple-samsung-example.txt**) with both NLTK and spaCy and discuss the similarities and differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who to contact for questions\n",
    "* Piek Vossen (piek.vossen@vu.nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tip: how to read a file from disk\n",
    "Let's open the file **Lab1-apple-samsung-example.txt** from disk. It should be located in the same folder as this notebook. The most simple way is to specify the full path to the file, e.g.:\n",
    "\n",
    "```\n",
    "path_to_file='/Users/piek/Desktop/t-ONDERWIJS/2021-2022/t-MA-HLT-introduction-2021/ma-hlt-labs/lab1.toolkits'\n",
    "```\n",
    "\n",
    "This may work for me but not for you as it is unlikely that the file has the same path on your machine.\n",
    "\n",
    "We can use the Path module to find the directory of this notebook. Once we have that, we only need to concatenate the name of the text file to this path. This is how you do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory of this notebook: /Users/piek/Downloads/ma-hlt-labs-local/lab1.toolkits\n",
      "Path to the text file: /Users/piek/Downloads/ma-hlt-labs-local/lab1.toolkits/Lab1-apple-samsung-example.txt\n"
     ]
    }
   ],
   "source": [
    "cur_dir = Path().resolve() # this should provide you with the folder in which this notebook is placed\n",
    "print('Current directory of this notebook:', cur_dir)\n",
    "\n",
    "## We can now stick the name of the file to the end of the Path using the *joinpath* function:\n",
    "path_to_file = Path.joinpath(cur_dir, 'Lab1-apple-samsung-example.txt')\n",
    "print('Path to the text file:', path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are unsure whether the path is correct, you can check if the file exist on that location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does path exist? -> True\n"
     ]
    }
   ],
   "source": [
    "print('does path exist? ->', Path.exists(path_to_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output from the code cell above says: **does path exist? -> False**, please check that the file **Lab1-apple-samsung-example.txt** is in the same directory as this notebook. In Jupyter lab you shopuld see it in the file overview panel to the left next to the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can open the file and access its content. Lets read the complete content and ask for it length using the 'len' function, which will tell us how many characters a string has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters 1139\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as infile:\n",
    "    text = infile.read()\n",
    "\n",
    "print('number of characters', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.telegraph.co.uk/technology/apple/9702716/Apple-Samsung-lawsuit-six-more-products-under-scrutiny.html\n",
      "\n",
      "Documents filed to the San Jose federal court in California on November 23 list six Samsung products running the \"Jelly Bean\" and \"Ice Cream Sandwich\" operating systems, which Apple claims infringe its patents.\n",
      "The six phones and tablets affected are the Galaxy S III, running the new Jelly Bean system, the Galaxy Tab 8.9 Wifi tablet, the Galaxy Tab 2 10.1, Galaxy Rugby Pro and Galaxy S III mini.\n",
      "Apple stated it had “acted quickly and diligently\" in order to \"determine that these newly released products do infringe many of the same claims already asserted by Apple.\"\n",
      "In August, Samsung lost a US patent case to Apple and was ordered to pay its rival $1.05bn (£0.66bn) in damages for copying features of the iPad and iPhone in its Galaxy range of devices. Samsung, which is the world's top mobile phone maker, is appealing the ruling.\n",
      "A similar case in the UK found in Samsung's favour and ordered Apple to publish an apology making clear that the South Korean firm had not copied its iPad when designing its own devices.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now created a string object with the name 'text' that we can use for the assignment below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some reason, you see weird characters in the text you may have a problem with the character encoding. Computers use different systems to represent scripts. For most languages UTF-8 will work as it has representations for many different characters. In some cases, especially older texts, Latin encodings have been used which works for English and some languages but cannot represent characters in others. For non-Western scripts special encodings have been defined. You never know for sure what encoding a text is in but now-adays most texts are in UTF-8.\n",
    "\n",
    "## What to do if you see weird tokens?\n",
    "First check if you are really using Python 3.x and not Python 2.x when running the notebook. You can do this using: \n",
    "\n",
    "    import platform\n",
    "    print(platform.python_version())\n",
    "\n",
    "If your are running 3.x and still have encoding problems try to open the file as utf-8:\n",
    "\n",
    "    with open(path_to_file, encoding=‘utf-8') as infile:\n",
    "    \n",
    "Note that when you open a text file in a plain text editor, you never know how it loads the file. The weird characters may still be there or disappear. In some cases, you can try to save the text file again using UTF-8 but this can also corrupt your file. It is wise to make a copy of the file before you try this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [total points: 4] Exercise 1: NLTK\n",
    "In this exercise, we use NLTK to apply **Part-of-speech (POS) tagging**, **Named Entity Recognition (NER)**, and **Constituency parsing**. The following code snippet already performs sentence splitting and tokenization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_nltk = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_sentence = [] # this will become a list of lists!!!\n",
    "\n",
    "#Below you find a so-called for loop\n",
    "for sentence_nltk in sentences_nltk:\n",
    "    sent_tokens = word_tokenize(sentence_nltk)\n",
    "    # We append the tokens of this sentence to the result list\n",
    "    tokens_per_sentence.append(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use lists to keep track of the output of the NLP tasks. We can hence inspect the output for each task using the index of the sentence. Lets look at the first sentence. Since the text starts with the URL to the source, our first real sentence is index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE The six phones and tablets affected are the Galaxy S III, running the new Jelly Bean system, the Galaxy Tab 8.9 Wifi tablet, the Galaxy Tab 2 10.1, Galaxy Rugby Pro and Galaxy S III mini.\n",
      "TOKENS ['The', 'six', 'phones', 'and', 'tablets', 'affected', 'are', 'the', 'Galaxy', 'S', 'III', ',', 'running', 'the', 'new', 'Jelly', 'Bean', 'system', ',', 'the', 'Galaxy', 'Tab', '8.9', 'Wifi', 'tablet', ',', 'the', 'Galaxy', 'Tab', '2', '10.1', ',', 'Galaxy', 'Rugby', 'Pro', 'and', 'Galaxy', 'S', 'III', 'mini', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence_id = 1\n",
    "print('SENTENCE', sentences_nltk[sentence_id])\n",
    "print('TOKENS', tokens_per_sentence[sentence_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With sentence=1 we get the second line from the text. Next you can define any value to sentence_id to carry out the assignment. Also try out sentence_id=0. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the assignment, pick a sentence you think has interesting properties and define the value for sentence_id correspondingly. In this notebook it is set to sentence '2' which is the third sentence. Change it to get your sentence and continue with the assignment in which you will use this value for sentence_id from now on. So do not pick out a short sentence without punctuation or any named entities!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_id=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain here in words why you selected this sentence! Why do you expect it to give interesting results?\n",
    "[your explanation goes here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [point: 1] Exercise 1a: Part-of-speech (POS) tagging\n",
    "Use *nltk.pos_tag* to perform part-of-speech tagging on a single sentence.\n",
    "\n",
    "Use **print** to show the output in the notebook (and hence also in the exported PDF!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = tokens_per_sentence[sentence_id]\n",
    "pos_tagged_sentence_tokens= [] \n",
    "#put here the call to nltk pos tagger and to assign the result to the variable 'pos_tagged_sentence_tokens'\n",
    "print(pos_tagged_sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your analysis\n",
    "[Look at the output and comment on the Part-of-Speech tags. Your comments go here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [point: 1] Exercise 1b: Named Entity Recognition (NER)\n",
    "Use *nltk.chunk.ne_chunk* to perform Named Entity Recognition (NER) on your selected sentence.\n",
    "\n",
    "Use **print** to show the output in the notebook (and hence also in the exported PDF!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "tokens_pos_tagged_and_named_entities = []\n",
    "#put here the call to nltk named entity chunker and to assign the result to the variable 'tokens_pos_tagged_and_named_entities'\n",
    "print(tokens_pos_tagged_and_named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your analysis\n",
    "[Look at the output and comment on the entities detected and labeled. Your comments go here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [points: 2] Exercise 1c: Constituency parsing\n",
    "Use the *nltk.RegexpParser* to perform constituency parsing on your selected sentence. Think about what the parsing expects as input.\n",
    "\n",
    "Use **print** to show the output in the notebook (and hence also in the exported PDF!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituent_parser_v1 = nltk.RegexpParser('''\n",
    "NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "P: {<IN>}           # Preposition\n",
    "V: {<V.*>}          # Verb\n",
    "PP: {<P> <NP>}      # PP -> P NP\n",
    "VP: {<V> <NP|PP>*}  # VP -> V (NP|PP)*''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituency_v1_output_for_sentence = []\n",
    "#add here your code to assign the output of the parser 'constituent_parser_v1' to the variable name 'constituency_v1_output_for_sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(constituency_v1_output_for_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your analysis\n",
    "[Look at the output and comment on the structures detected and labels assigned. Explain this by commenting on the grammar]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment the RegexpParser so that it also detects Named Entity Phrases (NEP), e.g., that it detects phrases such as *Galaxy S III* and *Ice Cream Sandwich* as entity phrases, which we give the label 'NEP'. Below you see an empty structure {} for NEP and ??? as comment. Fill the empty structure with a pattern to detect NEPs and write your comment in the code to explain what you have done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "You should apply the grammar to the output of the PoS tagging and not the output of the ne_chunker!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituent_parser_v2 = nltk.RegexpParser('''\n",
    "NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "P: {<IN>}           # Preposition\n",
    "V: {<V.*>}          # Verb\n",
    "PP: {<P> <NP>}      # PP -> P NP\n",
    "VP: {<V> <NP|PP>*}  # VP -> V (NP|PP)*\n",
    "NEP: {}             # ???''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituency_v2_output_for_sentence = []\n",
    "#add here your code to assign the output of the parser 'constituent_parser_v2' to the variable name 'constituency_v2_output_for_sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(constituency_v2_output_for_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your analysis\n",
    "[Compare the output of your grammar with the ne_chunker output. Which has better coverage and which has richer labels? How could you improve your grammar?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [total points: 1] Exercise 2: spaCy\n",
    "Use Spacy to process the same text as you analyzed with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text) # insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: You can use **sents = list(doc.sents)** to be able to use the index to access a sentence like **sents[2]** for the third sentence. Use the previsouly defined \"sentence_id\" to get the output for the same NLTK sentence. Note that we assume that the sentences are split in the same way. So lets get the spaCy sentence for sentence_id and use this to compare against the NLTK output we had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "In August, Samsung lost a US patent case to Apple and was ordered to pay its rival $1.05bn (£0.66bn) in damages for copying features of the iPad and iPhone in its Galaxy range of devices.\n"
     ]
    }
   ],
   "source": [
    "sents=list(doc.sents)\n",
    "spacy_sentence=sents[sentence_id]\n",
    "print(spacy_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [total points: 5] Exercise 3: Comparison NLTK and spaCy\n",
    "We will now compare the output of NLTK and spaCy for the same sentence identified with sentence_id and which you just processed with the NLTK, i.e. in what do they differ? So here we expect you to critically think about at the differences in output and not just run the code and describe the differences. What is good and what is bad and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [points: 2] Exercise 3a: Part of speech tagging\n",
    "### Your analysis\n",
    "\n",
    "Compare the output from NLTK and Spacy regarding part of speech tagging for the selected sentence. You already had the PoS Tags from NLTK. Get the tokens and their PoS tags (**token.tag**) from spaCy. Print both and describe any differences. This is not a trick question, it is possible that there are no differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "In August, Samsung lost a US patent case to Apple and was ordered to pay its rival $1.05bn (£0.66bn) in damages for copying features of the iPad and iPhone in its Galaxy range of devices.\n"
     ]
    }
   ],
   "source": [
    "sentence=sents[sentence_id]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [points: 1] Exercise 3b: Named Entity Recognition (NER)\n",
    "* For the same sentence, describe differences between the output from NLTK and spaCy for Named Entity Recognition. Which one do you think performs better?\n",
    "\n",
    "### Your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of the assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
