{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to spaCy\n",
    "\n",
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "\n",
    "Whereas NLTK contains separate modules that need to be called individually, we now look at another toolkit that hides these steps in one processing module. Basically, the toolkit takes the plain text as input, applies a whole series of NLP modules to the text and outputs a complex object with the output of these modules stacked on top of the elements of the text.\n",
    "\n",
    "[spaCy](https://spacy.io/) provides a rather complete NLP pipeline (the output of one module feeds to the input of the next): it takes a raw document and performs tokenization, POS-tagging, stop word recognition, morphological analysis, lemmatization, sentence splitting, dependency parsing and Named Entity Recognition (NER), among others. It also supports similarity prediction, but that is outside of the scope of this notebook. The advantage of spaCy is that it is really fast, and it has a reasonable accuracy. In addition, it currently supports multiple languages: https://spacy.io/models.\n",
    "\n",
    "So whereas in NLTK you built your own pipeline, spaCy is for lazy people and can do it all for you. That is not completely correct as you can build your own pipeline also in spaCy but that is not so trivial and most people are lazy.\n",
    "\n",
    "In this notebook, we will show you the basic usage. If you want to learn more, please visit spaCy's website; it has extensive documentation and provides excellent user guides. \n",
    "\n",
    "**At the end of this notebook, you will be able to extract the output from spaCy for the following NLP tasks**:\n",
    "* **Sentence splitting**: attribute **sents** of a `Doc` (of type *spacy.tokens.doc.Doc*)\n",
    "* **Tokenization**: `Doc` contains a sequence of `Token` objects (of type *spacy.tokens.token.Token*)\n",
    "* **Part-of-speech (POS) tagging**: attributes **pos_** and **tag_** of `Token`\n",
    "* **Stop words recognition** attribute **is_stop** of `Token`\n",
    "* **Stemming and lemmatization**: attribute **lemma_** of `Token`\n",
    "* **Constituency/dependency parsing:** attributes **dep_** and **head**\n",
    "* **Named Entity Recognition (NER):** attribute **ents** (of type *spacy.tokens.span.Span*) of `Doc` (of type *spacy.tokens.doc.Doc*). \n",
    "\n",
    "In addition, you will be able to use spaCy to visualize the output for each NLP task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and loading spaCy\n",
    "\n",
    "To install spaCy, check out the instructions [here](https://spacy.io/usage). It explains exactly how to install spaCy for your operating system, package manager and desired languages. Simply run the suggested commands in your terminal ([Anaconda Prompt](https://docs.anaconda.com/anaconda/user-guide/getting-started/) or cmd). Alternatively, you can probably also just run the following cells in this notebook:\n",
    "\n",
    "**Tip**: comment out the next two commands after using them. You can comment out commands by putting a \"#\" in front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/piek/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - spacy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2022.6.15  |       h033912b_0         149 KB  conda-forge\n",
      "    catalogue-2.0.6            |   py39h6e9494a_2          31 KB  conda-forge\n",
      "    certifi-2022.6.15          |   py39h6e9494a_0         155 KB  conda-forge\n",
      "    conda-4.13.0               |   py39h6e9494a_1         986 KB  conda-forge\n",
      "    cymem-2.0.6                |   py39h9fcab8e_2          34 KB  conda-forge\n",
      "    cython-blis-0.7.8          |   py39h15b18c7_0         5.2 MB  conda-forge\n",
      "    langcodes-3.3.0            |     pyhd8ed1ab_0         156 KB  conda-forge\n",
      "    murmurhash-1.0.7           |   py39he9d5cce_0          22 KB\n",
      "    openssl-1.1.1p             |       hfe4f2af_0         1.9 MB  conda-forge\n",
      "    pathy-0.6.1                |     pyhd8ed1ab_0          37 KB  conda-forge\n",
      "    preshed-3.0.6              |   py39h9fcab8e_1          88 KB  conda-forge\n",
      "    pydantic-1.9.1             |   py39h701faf5_0         2.0 MB  conda-forge\n",
      "    python_abi-3.9             |           2_cp39           4 KB  conda-forge\n",
      "    shellingham-1.4.0          |     pyh44b312d_0          11 KB  conda-forge\n",
      "    smart_open-5.2.1           |     pyhd8ed1ab_0          43 KB  conda-forge\n",
      "    spacy-3.3.0                |   py39hc29d2bd_0         4.1 MB\n",
      "    spacy-legacy-3.0.9         |     pyhd8ed1ab_0          19 KB  conda-forge\n",
      "    spacy-loggers-1.0.2        |     pyhd8ed1ab_0          11 KB  conda-forge\n",
      "    srsly-2.4.3                |   py39he9d5cce_0         468 KB\n",
      "    thinc-8.0.15               |   py39hc29d2bd_0         642 KB\n",
      "    typer-0.4.1                |     pyhd8ed1ab_0          44 KB  conda-forge\n",
      "    wasabi-0.9.1               |     pyhd8ed1ab_0          25 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        16.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  catalogue          conda-forge/osx-64::catalogue-2.0.6-py39h6e9494a_2\n",
      "  cymem              conda-forge/osx-64::cymem-2.0.6-py39h9fcab8e_2\n",
      "  cython-blis        conda-forge/osx-64::cython-blis-0.7.8-py39h15b18c7_0\n",
      "  langcodes          conda-forge/noarch::langcodes-3.3.0-pyhd8ed1ab_0\n",
      "  murmurhash         pkgs/main/osx-64::murmurhash-1.0.7-py39he9d5cce_0\n",
      "  pathy              conda-forge/noarch::pathy-0.6.1-pyhd8ed1ab_0\n",
      "  preshed            conda-forge/osx-64::preshed-3.0.6-py39h9fcab8e_1\n",
      "  pydantic           conda-forge/osx-64::pydantic-1.9.1-py39h701faf5_0\n",
      "  python_abi         conda-forge/osx-64::python_abi-3.9-2_cp39\n",
      "  shellingham        conda-forge/noarch::shellingham-1.4.0-pyh44b312d_0\n",
      "  smart_open         conda-forge/noarch::smart_open-5.2.1-pyhd8ed1ab_0\n",
      "  spacy              pkgs/main/osx-64::spacy-3.3.0-py39hc29d2bd_0\n",
      "  spacy-legacy       conda-forge/noarch::spacy-legacy-3.0.9-pyhd8ed1ab_0\n",
      "  spacy-loggers      conda-forge/noarch::spacy-loggers-1.0.2-pyhd8ed1ab_0\n",
      "  srsly              pkgs/main/osx-64::srsly-2.4.3-py39he9d5cce_0\n",
      "  thinc              pkgs/main/osx-64::thinc-8.0.15-py39hc29d2bd_0\n",
      "  typer              conda-forge/noarch::typer-0.4.1-pyhd8ed1ab_0\n",
      "  typing-extensions  pkgs/main/noarch::typing-extensions-3.10.0.2-hd3eb1b0_0\n",
      "  wasabi             conda-forge/noarch::wasabi-0.9.1-pyhd8ed1ab_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2021.10.26~ --> conda-forge::ca-certificates-2022.6.15-h033912b_0\n",
      "  certifi            pkgs/main::certifi-2021.10.8-py39hecd~ --> conda-forge::certifi-2022.6.15-py39h6e9494a_0\n",
      "  conda              pkgs/main::conda-4.11.0-py39hecd8cb5_0 --> conda-forge::conda-4.13.0-py39h6e9494a_1\n",
      "  openssl              pkgs/main::openssl-1.1.1m-hca72f7f_0 --> conda-forge::openssl-1.1.1p-hfe4f2af_0\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? \n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2022.6.15    | 155 KB    | ########## | 100% \n",
      "openssl-1.1.1p       | 1.9 MB    | ########## | 100% \n",
      "smart_open-5.2.1     | 43 KB     | ########## | 100% \n",
      "preshed-3.0.6        | 88 KB     | ########## | 100% \n",
      "shellingham-1.4.0    | 11 KB     | ########## | 100% \n",
      "langcodes-3.3.0      | 156 KB    | ########## | 100% \n",
      "cython-blis-0.7.8    | 5.2 MB    | ########## | 100% \n",
      "murmurhash-1.0.7     | 22 KB     | ########## | 100% \n",
      "cymem-2.0.6          | 34 KB     | ########## | 100% \n",
      "ca-certificates-2022 | 149 KB    | ########## | 100% \n",
      "typer-0.4.1          | 44 KB     | ########## | 100% \n",
      "srsly-2.4.3          | 468 KB    | ########## | 100% \n",
      "spacy-3.3.0          | 4.1 MB    | ########## | 100% \n",
      "pathy-0.6.1          | 37 KB     | ########## | 100% \n",
      "conda-4.13.0         | 986 KB    | ########## | 100% \n",
      "spacy-loggers-1.0.2  | 11 KB     | ########## | 100% \n",
      "catalogue-2.0.6      | 31 KB     | ########## | 100% \n",
      "python_abi-3.9       | 4 KB      | ########## | 100% \n",
      "wasabi-0.9.1         | 25 KB     | ########## | 100% \n",
      "spacy-legacy-3.0.9   | 19 KB     | ########## | 100% \n",
      "pydantic-1.9.1       | 2.0 MB    | ########## | 100% \n",
      "thinc-8.0.15         | 642 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.11.0\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are no errors, you installed the software on your local machine. Congratulations!  But this is not enough. You also need to have the language resources to feed the software: trained models, grammars, lexicons, etc. spaCy comes with language resources for many different languages and this is growing. Maybe one day you may even contribute a module for your own language.\n",
    "\n",
    "In this notebook, we are going to download the English language resources. The standard donwload command from the command line is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 13.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.62.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: setuptools in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (58.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.22.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Using cached pydantic-1.8.2-cp39-cp39-macosx_10_9_x86_64.whl (2.7 MB)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/piek/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.0)\n",
      "Installing collected packages: pydantic, en-core-web-sm\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.1\n",
      "    Uninstalling pydantic-1.9.1:\n",
      "      Successfully uninstalled pydantic-1.9.1\n",
      "Successfully installed en-core-web-sm-3.3.0 pydantic-1.8.2\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the messages, you see that there is a download and install operation followed by a linking operation. There may be errors with the linking, which we try to solve below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's first load spaCy in the notebook and check if we can load the English language resources. We import the spaCy module and load the English tokenizer, tagger, parser, NER, and word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6w/bw7dqbl9727c48pcjjh32r140000gn/T/ipykernel_72763/1491521201.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# other languages: de, es, pt, fr, it, nl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp= spacy.load('en') # other languages: de, es, pt, fr, it, nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no message, you can assume that everything went well and you can skip the next part on errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to check the models that are installed for spaCy for their compatibility with the spaCy version that you are running.\n",
    "The next cell shows you how to do this on the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.3.0) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "/Users/piek/opt/anaconda3/lib/python3.9/site-packages/spacy\u001b[0m\n",
      "\n",
      "NAME              SPACY                 VERSION                            \n",
      "nl_core_news_sm   >=3.3.0.dev0,<3.4.0   \u001b[38;5;2m3.3.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "en_core_web_sm    >=3.3.0.dev0,<3.4.0   \u001b[38;5;2m3.3.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m spacy validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, there are two models and a link, all compatible with my spaCy. If there is no model listed, something went wrong with downloading. If spaCy lists a compatibility problem, it suggests how to fix it. Below are some more possible fixes. Note that these toolkits are improved rapidly and a new release may change things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible error when downloading language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might get a variation of the following error:\n",
    "```\n",
    "OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that there is a problem with linking the language model of spaCy. You can try to load it in the following way instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if this works, you might still want to invest some time to make sure that the linking was succesful, i.e., that you can load spaCy with spacy.load('en'). Here is some more information on how to fix this.\n",
    "\n",
    "**Troubleshooting (optional)**\n",
    "\n",
    "*Cause*: Anaconda prompt does not have enough priviliges to execute the linking part of `python -m spacy download en`. The same is true for any other `python -m spacy [...]` command.\n",
    "\n",
    "*Solution:* \n",
    "\n",
    "1. Create the link manually\n",
    "\n",
    "The prompt should display something along the lines of:\n",
    "\n",
    "```\n",
    "<Data Downloaded>\n",
    "You do not have sufficient privilege to perform this operation.\n",
    "\n",
    "    Linking successful\n",
    "    <Anaconda dir>\\lib\\site-packages\\en_core_web_sm -->\n",
    "    <Anaconda dir>\\lib\\site-packages\\spacy\\data\\en\n",
    "\n",
    "    You can now load the model via spacy.load('en')\n",
    "\n",
    "Use the following command:\n",
    "\n",
    "mklink /D <Anaconda>\\lib\\site-packages\\spacy\\data\\en <Anaconda>\\lib\\site-packages\\en_core_web_sm\n",
    "\n",
    " Note that the target is pointing to the link, not the other way around. The syntax of the arguments is 'target' followed by 'link'\n",
    " \n",
    "```\n",
    "\n",
    "2. Give Anaconda Permissions to create link. Using \"runas ... python -m spacy ...\" may not suffice\n",
    "\n",
    "3. More details: https://github.com/explosion/spaCy/issues/1283"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If none of this works (can happen on Windows)** You might want to install the model manually from SpaCy's GitHub through pip:\n",
    "\n",
    "```\n",
    "    pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still having problems? Come to the VU feedback sessions or schedule a zoom meeting with the teachers to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spaCy\n",
    "\n",
    "If you succesfully loaded the English model (or another language) with the above command, you created the spaCy object 'nlp'. \n",
    "\n",
    "An 'object'?????\n",
    "\n",
    "So what is an object? You can see it as an instance of a class of things. Whenever you create a variable and assign it a value of some type, this variable is a 'typed' instance. Like a baby that is born is a new instance of the type human and a kitten is a new instance of the type cat. It has all the properties of this type and you can do the things with the variable that were defined for that type, e.g. teach a baby a language which does not work for kittens.\n",
    "\n",
    "So 'nlp' is an instance of a spaCy object loaded with the models for the English language. This means that 'nlp' has all the functions that the spaCy developers defined.\n",
    "\n",
    "You can use 'nlp' now to process text through a defined pipeline of modules and store the result as a value for another variable for accessing it. The result of processing a text with spaCy is another spaCy object of the type 'Doc'.\n",
    "\n",
    "'Doc' objects are complex and they represent the interpretion of a text according to the standard functions in the spaCy pipeline. It will provides functions that give you access to different analyses that have been applied to the input text. In a Doc object you can access tokens that make up the text, their lemmas, their PoS, the sentences, chunks, named entities, and many more. Below, we will look at different the types of analyses in more detail.\n",
    "\n",
    "To process a text, you simple pass the string to the *nlp* object that we created as input and we assign the result of it to a variable *doc* with a lower case. The variable doc should then be of the tpe Doc. Let's check that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I have an awesome cat. She follows me through the house everywhere I go. Her name is shadow\")\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it. spaCy has now processed the text in many different ways ang gives you the sentences, tokens, dependency relations, named entities, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So whereas we had to call all these specific functions in the right order in the case of NLTK, i.e. sentence_splitting, tokenization, part-of-speech tagging, parsing and named entity recognition, spaCy did all of that when processing a text. The only thing you need to do is to access the output, which is what we will do next.\n",
    "\n",
    "`doc` is now a Python object of the class `Doc`. It is a container for accessing linguistic annotations and a sequence of `Token` objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc, Token and Span objects\n",
    "\n",
    "At this point, there are three important types of objects to remember:\n",
    "\n",
    "* A `Doc` is a sequence of `Token` objects.\n",
    "* A `Token` object represents an individual token — i.e. a word, punctuation symbol, whitespace, etc. It has attributes representing linguistic annotations. \n",
    "* A `Span` object is a slice from a `Doc` object and consists of a sequence of `Token` objects.\n",
    "\n",
    "Since `Doc` is a sequence of `Token` objects, we can iterate over all of the tokens in the text as shown below, or select a single token from the sequence. We need a 'for-loop' to iterate over the elements in the doc object just as we would do for a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "an\n",
      "awesome\n",
      "cat\n",
      ".\n",
      "She\n",
      "follows\n",
      "me\n",
      "through\n",
      "the\n",
      "house\n",
      "everywhere\n",
      "I\n",
      "go\n",
      ".\n",
      "Her\n",
      "name\n",
      "is\n",
      "shadow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the tokens using a for loop\n",
    "for token in doc:\n",
    "    print(token)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that spaCy does not really create a list but a so-called 'generator'. A generator is a so-called 'lazy iterator' in Python that does not overload memory:\n",
    "\n",
    "https://realpython.com/introduction-to-python-generators/\n",
    "\n",
    "You can turn it into a list however and load it in memory to see the content as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, have, an, awesome, cat, ., She, follows, me, through, the, house, everywhere, I, go, ., Her, name, is, shadow]\n"
     ]
    }
   ],
   "source": [
    "print(list(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with a list, we can also access each token individually using the item index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First token: I\n",
      "Second token: have\n"
     ]
    }
   ],
   "source": [
    "# Select one single token by index\n",
    "first_token = doc[0]\n",
    "print(\"First token:\", first_token)\n",
    "second_token = doc[1]\n",
    "print(\"Second token:\", second_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that even though these tokens look like strings, they are not. Print just gives the print representation of the token. We can also ask for the 'type' in Python and it will tell you what kind of object it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t <class 'spacy.tokens.token.Token'>\n",
      "have \t <class 'spacy.tokens.token.Token'>\n",
      "an \t <class 'spacy.tokens.token.Token'>\n",
      "awesome \t <class 'spacy.tokens.token.Token'>\n",
      "cat \t <class 'spacy.tokens.token.Token'>\n",
      ". \t <class 'spacy.tokens.token.Token'>\n",
      "She \t <class 'spacy.tokens.token.Token'>\n",
      "follows \t <class 'spacy.tokens.token.Token'>\n",
      "me \t <class 'spacy.tokens.token.Token'>\n",
      "through \t <class 'spacy.tokens.token.Token'>\n",
      "the \t <class 'spacy.tokens.token.Token'>\n",
      "house \t <class 'spacy.tokens.token.Token'>\n",
      "everywhere \t <class 'spacy.tokens.token.Token'>\n",
      "I \t <class 'spacy.tokens.token.Token'>\n",
      "go \t <class 'spacy.tokens.token.Token'>\n",
      ". \t <class 'spacy.tokens.token.Token'>\n",
      "Her \t <class 'spacy.tokens.token.Token'>\n",
      "name \t <class 'spacy.tokens.token.Token'>\n",
      "is \t <class 'spacy.tokens.token.Token'>\n",
      "shadow \t <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"\\t\", type(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `Token` objects have many useful *methods* and *attributes*, which we can get listed by using the Python function `dir()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(first_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't really talked about attributes during this course, but while functions are operations or actions performed by an object, attributes are 'static' features or properties of objects. Functions are called using parantheses (as we have seen with `str.split()`, for instance), while attributes go without parentheses. We will see some examples below. In the case of spaCy tokens, attributes typically contain *annotations* of the token in the text.\n",
    "\n",
    "You can find more detailed information about the token functions and attributes in the [documentation](https://spacy.io/api/token).\n",
    "\n",
    "Notice that there are many attributes with double listings, one without and once with the suffix `_`. The attributes without `_` actually have numerical values that spaCy uses internally, whereas variants with `_` have the human readable rendering of the value in unicode. The internal numerical repesentations are used to store data more efficiently, whereas the readable values are only generated for rendering output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690420944186131903 I 95 PRON\n",
      "14692702688101715474 have 100 VERB\n",
      "15099054000809333061 an 90 DET\n",
      "3240785716591152042 awesome 84 ADJ\n",
      "5439657043933447811 cat 92 NOUN\n",
      "12646065887601541794 . 97 PUNCT\n",
      "6740321247510922449 she 95 PRON\n",
      "14462500713227930305 follow 100 VERB\n",
      "4690420944186131903 I 95 PRON\n",
      "18216413589307435838 through 85 ADP\n",
      "7425985699627899538 the 90 DET\n",
      "9471806766518506264 house 92 NOUN\n",
      "10957650314384693728 everywhere 86 ADV\n",
      "4690420944186131903 I 95 PRON\n",
      "8004577259940138793 go 100 VERB\n",
      "12646065887601541794 . 97 PUNCT\n",
      "4115755726172261197 her 95 PRON\n",
      "18309932012808971453 name 92 NOUN\n",
      "10382539506755952630 be 87 AUX\n",
      "9756944156339232386 shadow 100 VERB\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.lemma, token.lemma_,token.pos, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use spacy.explain to find out more about certain labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out some more, such as NN, ADP, PRP, VBD, VBP, VBZ, WDT, aux, nsubj, pobj, dobj, npadvmod\n",
    "spacy.explain(\"ADP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence splitting & tokenization\n",
    "spaCy performs sentence splitting for you. The information is stored in the attribute **sents** of `Doc` (of type *spacy.tokens.doc.Doc*).\n",
    "Each `Doc` contains a sequence of `Token` objects, i.e., this is where the output from the tokenizer is found. The token itself can be accessed using the attribute **text**. Each `Doc` instance will also have an index over the tokens to group them into sentences. We can iterate over these sentence indexes and get the tokens from each sentence in sequence. This will access the text sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have an awesome cat. She follows me through the house everywhere I go. Her name is shadow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT SENTENCE\n",
      "I have an awesome cat.\n",
      "I\n",
      "have\n",
      "an\n",
      "awesome\n",
      "cat\n",
      ".\n",
      "NEXT SENTENCE\n",
      "She follows me through the house everywhere I go.\n",
      "She\n",
      "follows\n",
      "me\n",
      "through\n",
      "the\n",
      "house\n",
      "everywhere\n",
      "I\n",
      "go\n",
      ".\n",
      "NEXT SENTENCE\n",
      "Her name is shadow\n",
      "Her\n",
      "name\n",
      "is\n",
      "shadow\n"
     ]
    }
   ],
   "source": [
    "sentences=doc.sents\n",
    "for sentence in sentences:\n",
    "    print('NEXT SENTENCE')\n",
    "    print(sentence)\n",
    "    for token in sentence:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "The output from the lemmatizer is stored in the attribute **lemma_** of each `Token' object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have awesome cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats cat\n"
     ]
    }
   ],
   "source": [
    "cat_token = doc[3]\n",
    "print(cat_token.text, cat_token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the part of speech tagger is stored:\n",
    "* in the attribute **pos_** of each `Token` object: The simple part-of-speech tag\n",
    "* in the attribute **tag_** of each `Token object: The detailed part-of-speech tag ([Penn Treebank POS tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have awesome cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats NOUN NNS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'noun, plural'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_token = doc[3]\n",
    "print(cat_token.text, cat_token.pos_, cat_token.tag_)\n",
    "spacy.explain(cat_token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing\n",
    "The output of the dependency parser can only be accessed by combining the information from multiple attributes. Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has a special function *displacy* to display data. We first need to import it and next can apply it to the *doc* object. There are various paramters that can be said. You can read more in the spaCy documentation: https://spacy.io/usage/visualizers. Now, we choose the *style* 'dep' for depenedency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"06d85b3bbece405f909f61f541271097-0\" class=\"displacy\" width=\"1275\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Autonomous</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">cars</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">shift</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">insurance</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">liability</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">toward</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">manufacturers</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06d85b3bbece405f909f61f541271097-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06d85b3bbece405f909f61f541271097-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06d85b3bbece405f909f61f541271097-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06d85b3bbece405f909f61f541271097-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06d85b3bbece405f909f61f541271097-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06d85b3bbece405f909f61f541271097-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06d85b3bbece405f909f61f541271097-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06d85b3bbece405f909f61f541271097-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06d85b3bbece405f909f61f541271097-0-4\" stroke-width=\"2px\" d=\"M420,264.5 C420,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06d85b3bbece405f909f61f541271097-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,266.5 L933.0,254.5 917.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06d85b3bbece405f909f61f541271097-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06d85b3bbece405f909f61f541271097-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1090.0,266.5 L1098.0,254.5 1082.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that each token has a dependency relation with at least one other token. For example:\n",
    "* **cars** has an **amod** relation with **autonomous**\n",
    "* the main verb **shift** has an **nsubj** relation with **cars**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know what these relations mean, you can use **spacy.explain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjectival modifier'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('amod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy makes use of the terms **child** and **head** in their dependency parsing output.\n",
    "* a relation is always in one direction from a **child** to a **head**, e.g., *autonomous* is the child of *cars*\n",
    "* a head of a phrase can be the child of another token, e.g., *cars* is the child of *shift*\n",
    "* a token without a head is the root of the text or sentence (often the main verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following attributes are needed to access this information:\n",
    "* **dep_** provides the syntactic relation, e.g., *nsubj*\n",
    "* **head** provides the **head** of a `Token`, e.g., in the case of *autonomous* the head would be *cars*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous amod cars\n"
     ]
    }
   ],
   "source": [
    "autonomous_token = doc[0]\n",
    "print(autonomous_token, autonomous_token.dep_, autonomous_token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars nsubj shift\n"
     ]
    }
   ],
   "source": [
    "cars_token = doc[1]\n",
    "print(cars_token, cars_token.dep_, cars_token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save tree structure to SVG image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_structure = displacy.render(doc, jupyter=False, style='dep')\n",
    "\n",
    "output_path = 'spacy_tree_structure.svg'\n",
    "with open(output_path, 'w') as outfile:\n",
    "    outfile.write(tree_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "The output from the Named Entity Recognizer is stored in the attribute **ents** of `Doc`.\n",
    "The attribute **label_** and an **ent** (of type *spacy.tokens.span.Span*) contains the named entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"But Google is starting from behind. The company made a late push into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa software, which runs on its Echo and Dot devices, have clear leads in consumer adoption.\"\"\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">But \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is starting from behind. The company made a late push into hardware, and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple’s Siri\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", available on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPhones\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "’s \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Alexa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " software, which runs on its \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Echo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and Dot devices, have clear leads in consumer adoption.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Apple’s Siri ORG\n",
      "iPhones ORG\n",
      "Amazon ORG\n",
      "Alexa ORG\n",
      "Echo ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
