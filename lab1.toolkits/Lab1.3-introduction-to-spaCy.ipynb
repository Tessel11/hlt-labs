{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to spaCy\n",
    "\n",
    "Copyright, Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "\n",
    "Whereas NLTK contains separate modules that need to be called individually, we now look at another toolkit *spaCy* that hides these steps in one processing module. Basically, *spaCy* takes plain text as input, applies a whole series of NLP modules to the text and outputs a complex object with the output of these modules stacked on top of the elements of the text.\n",
    "\n",
    "So whereas we had to create a pipeline ourself in NLTK, [spaCy](https://spacy.io/) thus provides such a NLP pipeline (the output of one module feeds to the input of the next): it performs tokenization, POS-tagging, stop word recognition, morphological analysis, lemmatization, sentence splitting, dependency parsing and Named Entity Recognition (NER), among others. It also supports similarity prediction, but that is outside of the scope of this notebook. The advantage of spaCy is that it is really fast, and it has a reasonable accuracy. In addition, it currently supports multiple languages: https://spacy.io/models.\n",
    "\n",
    "In this notebook, we will mainly show how to call this pipeline and access the result of processing a text. If you want to learn more, please visit spaCy's website; it has extensive documentation and provides excellent user guides. \n",
    "\n",
    "**At the end of this notebook, you will be able to get the output from spaCy after it applied the following NLP pipeline**:\n",
    "\n",
    "* **Sentence splitting**: attribute **sents** of a `Doc` (of type *spacy.tokens.doc.Doc*)\n",
    "* **Tokenization**: `Doc` contains a sequence of `Token` objects (of type *spacy.tokens.token.Token*)\n",
    "* **Part-of-speech (POS) tagging**: attributes **pos_** and **tag_** of `Token`\n",
    "* **Stop words recognition** attribute **is_stop** of `Token`\n",
    "* **Stemming and lemmatization**: attribute **lemma_** of `Token`\n",
    "* **Constituency/dependency parsing:** attributes **dep_** and **head**\n",
    "* **Named Entity Recognition (NER):** attribute **ents** (of type *spacy.tokens.span.Span*) of `Doc` (of type *spacy.tokens.doc.Doc*). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and loading spaCy\n",
    "\n",
    "To install spaCy, check out the instructions [here](https://spacy.io/usage). It explains exactly how to install spaCy for your operating system, package manager and desired languages. Simply run the suggested commands in your terminal ([Anaconda Prompt](https://docs.anaconda.com/anaconda/user-guide/getting-started/) or cmd). Alternatively, you can probably also just run the following cells in this notebook:\n",
    "\n",
    "**Tip**: comment out the next two commands after using them. You can comment out commands by putting a \"#\" in front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/98/9e/302ddb4586707a49a6b9a9cb803ee0cb10f7f1af115ada0b48aaf21189ad/spacy-3.6.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading spacy-3.6.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (19 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.9-cp39-cp39-macosx_10_9_x86_64.whl (18 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.7-cp39-cp39-macosx_10_9_x86_64.whl (32 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.8-cp39-cp39-macosx_10_9_x86_64.whl (107 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy)\n",
      "  Using cached thinc-8.1.10-cp39-cp39-macosx_10_9_x86_64.whl (867 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/84/67/c50bd3df4d7dcf0fb748b8aacd1aae21f0ae356678fb09a8b5354f7052c6/srsly-2.4.7-cp39-cp39-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached srsly-2.4.7-cp39-cp39-macosx_10_9_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting pathy>=0.10.0 (from spacy)\n",
      "  Obtaining dependency information for pathy>=0.10.0 from https://files.pythonhosted.org/packages/b5/c3/04a002ace658133f5ac48d30258ed9ceab720595dc1ac36df02fe52018af/pathy-0.10.2-py3-none-any.whl.metadata\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy) (2.29.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy)\n",
      "  Obtaining dependency information for pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 from https://files.pythonhosted.org/packages/22/aa/24ef442d21b1ddafb52fae1e50ebe76cfaf36cb0c58a046fa6eb1d87310d/pydantic-1.10.11-cp39-cp39-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading pydantic-1.10.11-cp39-cp39-macosx_10_9_x86_64.whl.metadata (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy)\n",
      "  Using cached blis-0.7.9-cp39-cp39-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/63/05/2d7acb83610e0c5171dfcb4a2968d676800da8082fc456e284fa8e0924f4/confection-0.1.0-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Using cached spacy-3.6.0-cp39-cp39-macosx_10_9_x86_64.whl (6.9 MB)\n",
      "Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Using cached pydantic-1.10.11-cp39-cp39-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "Using cached srsly-2.4.7-cp39-cp39-macosx_10_9_x86_64.whl (492 kB)\n",
      "Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Using cached confection-0.1.0-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.1.0 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pydantic-1.10.11 spacy-3.6.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 thinc-8.1.10 typer-0.9.0 wasabi-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of an installation using *pip*, you can also try to install it using *conda*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are no errors, you installed the software on your local machine. Congratulations!  But this is not enough. You also need to have the language resources to feed the software: trained models, grammars, lexicons, etc. spaCy comes with language resources for many different languages and this is growing. Maybe one day you may even contribute a module for your own language.\n",
    "\n",
    "In this notebook, we are going to download the English language resources. The standard donwload command from the command line is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.29.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.11)\n",
      "Requirement already satisfied: jinja2 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the messages, you see that there is a download and install operation followed by a linking operation. There may be errors with the linking, which we try to solve below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's first load spaCy in the notebook and check if we can load the English language resources. We import the spaCy module and load the English tokenizer, tagger, parser, NER, and word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp= spacy.load('en_core_web_sm') # other languages: de, es, pt, fr, it, nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no message, you can assume that everything went well and you can skip the next part on errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to check the models that are installed for spaCy for their compatibility with the spaCy version that you are running.\n",
    "The next cell shows you how to do this on the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.6.0) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "/Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages/spacy\u001b[0m\n",
      "\n",
      "NAME             SPACY            VERSION                            \n",
      "en_core_web_sm   >=3.6.0,<3.7.0   \u001b[38;5;2m3.6.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, there are two models and a link, all compatible with my spaCy. If there is no model listed, something went wrong with downloading. If spaCy lists a compatibility problem, it suggests how to fix it. Below are some more possible fixes. Note that these toolkits are improved rapidly and a new release may change things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible error when downloading language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might get a variation of the following error:\n",
    "```\n",
    "OSError: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that there is a problem with linking the language model of spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to invest some time to make sure that the linking was succesful, i.e., that you can load spaCy with spacy.load('en'). Here is some more information on how to fix this.\n",
    "\n",
    "**Troubleshooting (optional)**\n",
    "\n",
    "*Cause*: Anaconda prompt does not have enough priviliges to execute the linking part of `python -m spacy download en`. The same is true for any other `python -m spacy [...]` command.\n",
    "\n",
    "*Solution:* \n",
    "\n",
    "1. Create the link manually\n",
    "\n",
    "The prompt should display something along the lines of:\n",
    "\n",
    "```\n",
    "<Data Downloaded>\n",
    "You do not have sufficient privilege to perform this operation.\n",
    "\n",
    "    Linking successful\n",
    "    <Anaconda dir>\\lib\\site-packages\\en_core_web_sm -->\n",
    "    <Anaconda dir>\\lib\\site-packages\\spacy\\data\\en\n",
    "\n",
    "    You can now load the model via spacy.load('en')\n",
    "\n",
    "Use the following command:\n",
    "\n",
    "mklink /D <Anaconda>\\lib\\site-packages\\spacy\\data\\en <Anaconda>\\lib\\site-packages\\en_core_web_sm\n",
    "\n",
    " Note that the target is pointing to the link, not the other way around. The syntax of the arguments is 'target' followed by 'link'\n",
    " \n",
    "```\n",
    "\n",
    "2. Give Anaconda Permissions to create link. Using \"runas ... python -m spacy ...\" may not suffice\n",
    "\n",
    "3. More details: https://github.com/explosion/spaCy/issues/1283"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If none of this works (can happen on Windows)** You might want to install the model manually from SpaCy's GitHub through pip:\n",
    "\n",
    "```\n",
    "    pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still having problems? Come to the VU feedback sessions or schedule a zoom meeting with the teachers to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spaCy\n",
    "\n",
    "If you succesfully loaded the English model (or another language) with the above command, you created a spaCy object 'nlp'. \n",
    "\n",
    "An 'object'????? So what is an object? \n",
    "\n",
    "An object is an instance of a class of something. An iPhone10 is a class of things. Your iPhone is an object which is an instance of the class iPhone13. My iPhone is an instance of a Samsung Galaxy S23. Both our phones are objects but they have different properties and can do different things defined by the classes iPhone12 and Samsung GS23.\n",
    "\n",
    "In programming this works the same way. I can create as many instances of spaCy modules in my notebook as a want: nlp1, nlp2, nlp3.  So 'nlp' is an instance of a spaCy object loaded with the models for the English language. This means that 'nlp' has all the functions that the spaCy developers defined.\n",
    "\n",
    "I can also create an instance by loading the Dutch language model for spaCy or a newer version of an English language model when it is released, e.g.:\n",
    "\n",
    "```\n",
    "nlp_nl = spacy.load('nl_core_web_sm')\n",
    "\n",
    "nlp_it = spacy.load('it_core_web_sm')\n",
    "```\n",
    "\n",
    "This would give us two more spaCy instances in addition to the English variant that we created before. All three instances `nlp`, `nlp_nl` and `nlp_it` are objects of the same class. This means all general function of spaCy are available to all three but there could be language specific differences due to the implementation of these functions.\n",
    "\n",
    "Below we will use 'nlp' to process text through a predefined pipeline of modules and store the result as a value for another variable for accessing it. To process a text, you simple pass the string to the *nlp* object that we created as input and we assign the result of it to a variable *doc* with a lower case. The variable doc should then be of the class Doc. Let's check that as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I have an awesome cat. She follows me through the house everywhere I go. Her name is Shadow\")\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of processing a text with spaCy is another spaCy object instance of the class 'Doc'.\n",
    "\n",
    "'Doc' objects are complex and they represent the interpretion of a text according to the standard functions in the spaCy pipeline. spaCy provide functions that let you access the output of the different analyses that have been applied to the input text. In a Doc object you can access tokens that make up the text, their lemmas, their PoS, the sentences, chunks, named entities, and many more. Below, we will look at different the types of analyses in more detail.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So whereas we had to call all these specific functions in the right order in the case of NLTK, i.e. sentence_splitting, tokenization, part-of-speech tagging, parsing and named entity recognition, spaCy did all of that when processing a text. The only thing you need to do is to access the output, which is what we will do next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc, Token and Span objects\n",
    "\n",
    "At this point, there are three important classes of output objects:\n",
    "\n",
    "* A `Doc` is a sequence of `Token` objects.\n",
    "* A `Token` object represents an individual token — i.e. a word, punctuation symbol, whitespace, etc. It has attributes representing linguistic annotations. \n",
    "* A `Span` object is a slice from a `Doc` object and consists of a sequence of `Token` objects.\n",
    "\n",
    "Since `Doc` is a sequence of `Token` objects, we can iterate over all of the tokens in the text as shown below, or select a single token from the sequence. We need a 'for-loop' to iterate over the elements in the doc object just as we would do for a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "an\n",
      "awesome\n",
      "cat\n",
      ".\n",
      "She\n",
      "follows\n",
      "me\n",
      "through\n",
      "the\n",
      "house\n",
      "everywhere\n",
      "I\n",
      "go\n",
      ".\n",
      "Her\n",
      "name\n",
      "is\n",
      "Shadow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the tokens using a for loop\n",
    "for token in doc:\n",
    "    print(token)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that spaCy does not really create a list but a so-called 'generator'. A generator is a so-called 'lazy iterator' in Python that does not overload memory:\n",
    "\n",
    "https://realpython.com/introduction-to-python-generators/\n",
    "\n",
    "You can turn it into a list however and load it in memory to see the content as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, have, an, awesome, cat, ., She, follows, me, through, the, house, everywhere, I, go, ., Her, name, is, Shadow]\n"
     ]
    }
   ],
   "source": [
    "print(list(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a list, we can access each token individually using the item index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First token: I\n",
      "Second token: have\n"
     ]
    }
   ],
   "source": [
    "# Select one single token by index\n",
    "first_token = doc[0]\n",
    "print(\"First token:\", first_token)\n",
    "second_token = doc[1]\n",
    "print(\"Second token:\", second_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that even though these tokens look like strings, they are not. *Print* just gives the print representation of the token. We can also ask for the 'type' in Python and it will tell you what class of object it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t <class 'spacy.tokens.token.Token'>\n",
      "have \t <class 'spacy.tokens.token.Token'>\n",
      "an \t <class 'spacy.tokens.token.Token'>\n",
      "awesome \t <class 'spacy.tokens.token.Token'>\n",
      "cat \t <class 'spacy.tokens.token.Token'>\n",
      ". \t <class 'spacy.tokens.token.Token'>\n",
      "She \t <class 'spacy.tokens.token.Token'>\n",
      "follows \t <class 'spacy.tokens.token.Token'>\n",
      "me \t <class 'spacy.tokens.token.Token'>\n",
      "through \t <class 'spacy.tokens.token.Token'>\n",
      "the \t <class 'spacy.tokens.token.Token'>\n",
      "house \t <class 'spacy.tokens.token.Token'>\n",
      "everywhere \t <class 'spacy.tokens.token.Token'>\n",
      "I \t <class 'spacy.tokens.token.Token'>\n",
      "go \t <class 'spacy.tokens.token.Token'>\n",
      ". \t <class 'spacy.tokens.token.Token'>\n",
      "Her \t <class 'spacy.tokens.token.Token'>\n",
      "name \t <class 'spacy.tokens.token.Token'>\n",
      "is \t <class 'spacy.tokens.token.Token'>\n",
      "Shadow \t <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"\\t\", type(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `Token` objects have many useful *methods* and *attributes*, which we can get listed by using the Python function `dir()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(first_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't really talked about attributes during this course, but while functions are operations or actions performed by an object, attributes are 'static' features or properties of objects. Functions are called using parantheses (as we have seen with `str.split()`, for instance), while attributes go without parentheses. We will see some examples below. In the case of spaCy tokens, attributes typically contain *annotations* of the token in the text.\n",
    "\n",
    "You can find more detailed information about the token functions and attributes in the [documentation](https://spacy.io/api/token).\n",
    "\n",
    "Notice that there are many attributes with double listings, one without and one with the suffix `_`. The attributes without `_` actually have numerical values that spaCy uses internally, whereas variants with `_` have the human readable rendering of the value in unicode. The internal numerical repesentations are used to store data more efficiently, whereas the readable values are only generated for rendering output. Usually, you want to use the attributes with the `_` suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690420944186131903 I 95 PRON\n",
      "14692702688101715474 have 100 VERB\n",
      "15099054000809333061 an 90 DET\n",
      "3240785716591152042 awesome 84 ADJ\n",
      "5439657043933447811 cat 92 NOUN\n",
      "12646065887601541794 . 97 PUNCT\n",
      "6740321247510922449 she 95 PRON\n",
      "14462500713227930305 follow 100 VERB\n",
      "4690420944186131903 I 95 PRON\n",
      "18216413589307435838 through 85 ADP\n",
      "7425985699627899538 the 90 DET\n",
      "9471806766518506264 house 92 NOUN\n",
      "10957650314384693728 everywhere 86 ADV\n",
      "4690420944186131903 I 95 PRON\n",
      "8004577259940138793 go 100 VERB\n",
      "12646065887601541794 . 97 PUNCT\n",
      "4115755726172261197 her 95 PRON\n",
      "18309932012808971453 name 92 NOUN\n",
      "10382539506755952630 be 87 AUX\n",
      "4250795299896093889 Shadow 96 PROPN\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.lemma, token.lemma_,token.pos, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use spacy.explain labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out some more, such as NN, ADP, PRP, VBD, VBP, VBZ, WDT, aux, nsubj, pobj, dobj, npadvmod\n",
    "spacy.explain(\"ADP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence splitting & tokenization\n",
    "spaCy performs sentence splitting for you. The information is stored in the attribute **sents** of `Doc` (of type *spacy.tokens.doc.Doc*).\n",
    "Each `Doc` contains a sequence of `Token` objects, i.e., this is where the output from the tokenizer is found. The token itself can be accessed using the attribute **text**. Each `Doc` instance will also have an index over the tokens to group them into sentences. We can iterate over these sentence indexes and get the tokens from each sentence in sequence. This will access the text sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"I have an awesome cat. She follows me through the house everywhere I go. Her name is Shadow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEXT SENTENCE\n",
      "I have an awesome cat.\n",
      "I\n",
      "have\n",
      "an\n",
      "awesome\n",
      "cat\n",
      ".\n",
      "NEXT SENTENCE\n",
      "She follows me through the house everywhere I go.\n",
      "She\n",
      "follows\n",
      "me\n",
      "through\n",
      "the\n",
      "house\n",
      "everywhere\n",
      "I\n",
      "go\n",
      ".\n",
      "NEXT SENTENCE\n",
      "Her name is Shadow\n",
      "Her\n",
      "name\n",
      "is\n",
      "Shadow\n"
     ]
    }
   ],
   "source": [
    "sentences=doc.sents\n",
    "for sentence in sentences:\n",
    "    print('NEXT SENTENCE')\n",
    "    print(sentence)\n",
    "    for token in sentence:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "The output from the lemmatizer is stored in the attribute **lemma_** of each `Token' object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"I have awesome cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats cat\n"
     ]
    }
   ],
   "source": [
    "cat_token = doc[3]\n",
    "print(cat_token.text, cat_token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the part of speech tagger is stored:\n",
    "* in the attribute **pos_** of each `Token` object: The simple part-of-speech tag\n",
    "* in the attribute **tag_** of each `Token object: The detailed part-of-speech tag ([Penn Treebank POS tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"I have awesome cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats NOUN NNS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'noun, plural'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_token = doc[3]\n",
    "print(cat_token.text, cat_token.pos_, cat_token.tag_)\n",
    "spacy.explain(cat_token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing\n",
    "The output of the dependency parser can only be accessed by combining the information from multiple attributes. Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has a special function *displacy* to display data. We first need to import it and next can apply it to the *doc* object. There are various paramters that can be said. You can read more in the spaCy documentation: https://spacy.io/usage/visualizers. Now, we choose the *style* 'dep' for depenedency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c5d548f1180f46b18ebb525db9862ab5-0\" class=\"displacy\" width=\"1275\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Autonomous</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">cars</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">shift</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">insurance</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">liability</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">toward</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">manufacturers</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c5d548f1180f46b18ebb525db9862ab5-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c5d548f1180f46b18ebb525db9862ab5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c5d548f1180f46b18ebb525db9862ab5-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c5d548f1180f46b18ebb525db9862ab5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c5d548f1180f46b18ebb525db9862ab5-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c5d548f1180f46b18ebb525db9862ab5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c5d548f1180f46b18ebb525db9862ab5-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c5d548f1180f46b18ebb525db9862ab5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c5d548f1180f46b18ebb525db9862ab5-0-4\" stroke-width=\"2px\" d=\"M420,264.5 C420,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c5d548f1180f46b18ebb525db9862ab5-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,266.5 L933.0,254.5 917.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c5d548f1180f46b18ebb525db9862ab5-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c5d548f1180f46b18ebb525db9862ab5-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1090.0,266.5 L1098.0,254.5 1082.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that each token has a dependency relation with at least one other token. For example:\n",
    "* **cars** has an **amod** relation with **autonomous**\n",
    "* the main verb **shift** has an **nsubj** relation with **cars**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know what these relations mean, you can use **spacy.explain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjectival modifier'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('amod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy makes use of the terms **child** and **head** in their dependency parsing output.\n",
    "* a relation is always in one direction from a **child** to a **head**, e.g., *autonomous* is the child of *cars*\n",
    "* a head of a phrase can be the child of another token, e.g., *cars* is the child of *shift*\n",
    "* a token without a head is the root of the text or sentence (often the main verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following attributes are needed to access this information:\n",
    "* **dep_** provides the syntactic relation, e.g., *nsubj*\n",
    "* **head** provides the **head** of a `Token`, e.g., in the case of *autonomous* the head would be *cars*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous amod cars\n"
     ]
    }
   ],
   "source": [
    "autonomous_token = doc[0]\n",
    "print(autonomous_token, autonomous_token.dep_, autonomous_token.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars nsubj shift\n"
     ]
    }
   ],
   "source": [
    "cars_token = doc[1]\n",
    "print(cars_token, cars_token.dep_, cars_token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save tree structure to SVG image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_structure = displacy.render(doc, jupyter=False, style='dep')\n",
    "\n",
    "output_path = 'spacy_tree_structure.svg'\n",
    "with open(output_path, 'w') as outfile:\n",
    "    outfile.write(tree_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "The output from the Named Entity Recognizer is stored in the attribute **ents** of `Doc`.\n",
    "The attribute **label_** and an **ent** (of type *spacy.tokens.span.Span*) contains the named entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"But Google is starting from behind. The company made a late push into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa software, which runs on its Echo and Dot devices, have clear leads in consumer adoption.\"\"\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">But \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is starting from behind. The company made a late push into hardware, and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "’s \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Siri\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", available on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPhones\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "’s \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Alexa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " software, which runs on its \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Echo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " and Dot devices, have clear leads in consumer adoption.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Apple ORG\n",
      "Siri PERSON\n",
      "iPhones ORG\n",
      "Amazon ORG\n",
      "Alexa ORG\n",
      "Echo LOC\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
