{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB2.3 Creating embeddings from text in some language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to show how you can create word embeddings from a text collection for a specific language:\n",
    "\n",
    "<ol>\n",
    "<li>Obtain a text corpus from the web. We will use the Leipzig Corpora Collection that contains texts in many languages and was already preprocessed.\n",
    "<li>Tokenize the text to get the individual words in sentences as a list. We use the NLTK toolkit and a specific tokenization function to do that.\n",
    "<li>Create an embedding model from the tokenized text using the Gensim package\n",
    "<li>Demonstrate how to use the embedding model\n",
    "<li>Show how the word embedding space can be visualised\n",
    "</ol>\n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/index.html#core-tutorials-new-users-start-here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtaining text from the Leipzig Corpora collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Leipzig corpora collection has corpora for over 250 languages. These corpora are collected from Wikipedia, news and web crawls. Download a corpus in a language from:\n",
    "\n",
    "http://wortschatz.uni-leipzig.de/en/download/\n",
    "\n",
    "We will use the 'eng_news_2005_1M-text' corpus in this notebook. \n",
    "\n",
    "Unpack the compressed file somewhere on your computer. You will see it contains a number of files in a folder that have been created by the Leipzig NLP group from the sources. For example, the files \"...-sources.txt\" contain the list of URLs from which the text was obtained preceded by an identifier and followed by the date of crawling:\n",
    "\n",
    "```\n",
    "1\thttp://davesipaq.com/articles/iPAQ_Plustek_portable_scanning_solution.html\t2005-06-12\n",
    "2\thttp://www.independent.com/cover/Cover959.htm\t2005-04-08\n",
    "3\thttp://www.insidebayarea.com/ci_2736737?rss\t2005-05-15\n",
    "4\thttp://www.dailycollegian.com/vnews/display.v/ART/2005/05/13/4282dbfadd830\t2005-05-12\n",
    "5\thttp://p2pnet.net/story/4856\t2005-05-16\n",
    "6\thttp://www.imf.org/external/np/tr/2005/tr050324a.htm\t2005-04-09\n",
    "```\n",
    "\n",
    "The \"...-words.txt\" file contains the vocabulary of words with their frequency, e.g.:\n",
    "\n",
    "```\n",
    "452\tlaw\t5521\n",
    "453\tmaking\t5514\n",
    "454\trecord\t5511\n",
    "455\twhether\t5496\n",
    "456\ttimes\t5488\n",
    "457\tSt.\t5485\n",
    "458\tscored\t5484\n",
    "459\ttaken\t5484\n",
    "```\n",
    "\n",
    "We are going to use the file named \"...-sentences.txt\", which contain a sentence on each line preceded by an identifier, e.g.:\n",
    "\n",
    "```\n",
    "1\tI didn't know it was police housing,\" officers quoted Tsuchida as saying.\n",
    "2\tYou would be a great client for Southern Indiana Homeownership's credit counseling but you are saying to yourself \"Oh, we can pay that off.\"\n",
    "3\tHe believes the 21st century will be the \"century of biology\" just as the 20th century was the century of IT.\n",
    "4\tThey even call the civil rights organization a bit hypocritical.\n",
    "```\n",
    "\n",
    "Our goal is to use these sentences to create word embeddings. To be able to do that we need to process this file line by line, obtain the tokens from each sentence and separate punctuation from each token. We are going to do this with the NLTK toolkit and define a specific function called 'preprocess_rawtext' that does all the work for us.\n",
    "\n",
    "What is a function? A function is an ordered sequence of commands packaged into a group (like a recipe) with a name and possibly parameters between round brackets. So far you have been calling functions for instances of classes such as string or list that have been defined by other programners. You can however also define functions yourself. This is specifically useful if:\n",
    "\n",
    "<ol>\n",
    "<li> the code becomes too long and you want to group smaller steps into higher level steps without bothering about what happens inside: e.g. like playing music instead of pushing piano keys\n",
    "<li> code needs to be applied more than once and you do not want to repeat the code and make sure it is consistent across the repeated calls.\n",
    "</ol>\n",
    "    \n",
    "The function that we define below calls other functions as well that we also need to define, so you can see it is definitely a higher-order function. \n",
    "\n",
    "Once defined, we only need to apply this function to a local file on our disk to carry out a whole series of instructions and we can easliy do this many times for all kinds of files in the same format, e.g. downloaded from the Leipzig website. The function guarantees that the same process is applied each time.\n",
    "\n",
    "The next cell contains the processing function. After your run the cell in your notebook, the function is available to do the work for you. This means it is defined but it has not been used yet. For that we need to apply it to something. We do that later.\n",
    "\n",
    "For now, you can try to read and understand the function or just call it when you need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We use the NLTK tokenization function to process the text\n",
    "# For this we import the modules word_tokenize and sent_tokenize\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "\n",
    "#Function to remove punctuation from word tokens, \n",
    "#Takes a list of tokens as input\n",
    "\n",
    "#Note that these functions only work if you also imported NLTK and string before calling the function\n",
    "def remove_punct(tokens):\n",
    "    # punct is a string with all punctuation tokens: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    punct = string.punctuation\n",
    "    # empty list in which we put the clean tokens\n",
    "    tokens_clean = []\n",
    "\n",
    "    # Iterate over all characters in tokens \n",
    "    # and only keeps them if not in punct\n",
    "    for t in tokens:\n",
    "        if t not in punct:\n",
    "            tokens_clean.append(t)\n",
    "    # The result is a list with the cleaned tokens\n",
    "    return tokens_clean\n",
    "\n",
    "# The Leipzig corpus is already processed into sentences, so we do not need to split the text into sentences\n",
    "# We can read it line by line but \n",
    "# we need to skip the first token in each line which is the identifier and not regular text\n",
    "\n",
    "# Takes as input parameter the path to a file\n",
    "def preprocess_leipzig_sentences(file):\n",
    "    clean_sentences = []\n",
    "    \n",
    "    with open(file, \"r\") as i:\n",
    "            for sentence in i:\n",
    "                # We downcase each sentence, word_tokenize it with NLTK\n",
    "                tokens = word_tokenize(sentence.lower())\n",
    "                # We apply our custom remove_punct function and exclude the first token\n",
    "                tokens_clean = remove_punct(tokens[1:]) # we skip the first token which is the identifier.\n",
    "                # We add the clean tokens as a list to the list of sentences\n",
    "                clean_sentences.append(tokens_clean)\n",
    "                \n",
    "    # The result is a list of lists, each representing the tokens of a sentence as elements\n",
    "    return clean_sentences\n",
    "\n",
    "# If you want to process other text than the Leipzig corpus that is not split into sentences,\n",
    "# you can call the next function. The difference is:\n",
    "# - we read the complete file as a text string\n",
    "# - we apply the NLTK sent_tokenize function to the get a list of sentences\n",
    "# - we do not need to remove the identifier\n",
    "def preprocess_rawtext(file):\n",
    "    clean_sentences = []\n",
    "\n",
    "    with open(file) as infile:\n",
    "        text = infile.read()\n",
    "        \n",
    "    sentences = sent_tokenize(text.strip())\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        tokens_clean = remove_punct(tokens)\n",
    "        clean_sentences.append(tokens_clean)\n",
    "    return clean_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the above custom function to the Leipzig text corpus file with the sentences.\n",
    "\n",
    "You need to adapt the path_to_the_corpus_file to the correct location of the file on your computer.\n",
    "If the path is wrong you get an error message!\n",
    "\n",
    "It takes a while before the whole file is processed. Get a coffee or cup of tea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#eng_news_2005_1M-sentences.txt\n",
    "path_to_the_corpus_file='/Users/piek/Desktop/t-ONDERWIJS/data/leipzig-corpora/eng_news_2005_1M-text/eng_news_2005_1M-sentences.txt'\n",
    "text_leipzigcorpus_clean = preprocess_leipzig_sentences(path_to_the_corpus_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect text_leipzigcorpus_clean by asking for its length and printing a small sample, in this case sentence 201 till 208. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences= 1000000\n",
      "[['his', 'forehead', 'is', 'fractured', 'in', 'several', 'places', 'and', 'his', 'brain', 'and', 'one', 'of', 'his', 'lungs', 'are', 'bruised', 'she', 'said'], [\"''\", 'their', 'reputation', 'is', 'totally', 'vindicated', \"''\", 'loevy', 'said'], ['he', 'also', 'was', 'administratively', 'charged', 'with', 'breaking', 'state', 'law', 'lying', 'and', 'failing', 'to', 'report', 'information', 'to', 'the', 'department', 'in', 'the', 'jude', 'beating'], [\"''\", 'the', 'mta', 'were', 'directed', 'to', 'make', 'certain', 'amendments', 'to', 'their', 'constitution', 'to', 'ensure', 'clubs', 'are', 'directly', 'affiliated', 'to', 'the', 'national', 'body', 'with', 'voting', 'rights', \"''\", 'said', 'elyas'], ['both', 'last', 'raced', 'in', 'the', 'florida', 'derby', 'on', 'april', '2'], ['they', 'were', 'fifth', 'last', 'year', 'in', 'prague', 'fourth', 'in', '2003', 'at', 'helsinki', 'and', 'fifth', 'in', '2002', 'at', 'goteborg', 'sweden'], ['nicklaus', 'said', 'farewell', 'to', 'the', 'us', 'masters', 'a', 'tournament', 'he', 'won', 'a', 'record', 'six', 'times', 'last', 'month', 'carding', 'a', 'second', 'round', '76', 'to', 'miss', 'the', 'cut', 'by', 'five', 'strokes']]\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences=',len(text_leipzigcorpus_clean))\n",
    "#we print a few sentences to see how it looks like\n",
    "print(text_leipzigcorpus_clean[201:208])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a language model with word embeddings, we will use the **gensim** package again. \n",
    "\n",
    "In order to build the word embeddings through gensim, we are going to use the Word2Vec function that is included in gensim. Word2Vec is the Google package that provided a break-through in the performance of embeddings (Mikolov et al. 2013). Check its citations in Google scholar!\n",
    "\n",
    "    Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. \n",
    "    \"Distributed representations of words and phrases and their compositionality.\" \n",
    "    In Advances in neural information processing systems, pp. 3111-3119. 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our embeddings, gensim allows us to set a number of parameters. The most important of these are `min_count`, `window`, `size` and `sg`:\n",
    "\n",
    "* `min_count` is the minimum frequency of the words in our corpus. For infrequent words, we just don't have enough information to train reliable word embeddings. It therefore makes sense to set this minimum frequency to at least 10. In these experiments, we'll set it to 100 to limit the size of our model even more and to speed up things.\n",
    "* `window` is number of words to the left and to the right that make up the context that word2vec will take into account to make predictions.\n",
    "* `size` is the dimensionality of the word vectors. This is generally between 100 and 500. You often have to make a trade-off: embeddings with a higher dimensionality are able to model more information, but also need more data to train.\n",
    "* `sg`: there are two algorithms to train word2vec: skip-gram and CBOW. Skip-gram tries to predict the context on the basis of the target word; CBOW tries to find the target on the basis of the context. By default, Gensim uses CBOW (`sg=0`).\n",
    "\n",
    "We'll investigate the impact of some of these parameters later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next command creates an embedding model from our cleaned corpus. The model is assigned to the variable 'englishleipzig_w2v'(any name will do) and can be used next in this notebook. We also save the embedding model to disk as 'txt' file and as 'binary' data (bin) so that we can load it later and do not need to build the model over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You need to do the next commands only once. When you have succesfully created and saved the embeddings you can load them afterwards\n",
    "from gensim.models import Word2Vec\n",
    "englishleipzig_w2v = Word2Vec(text_leipzigcorpus_clean, vector_size=100, window =4, min_count =100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model has a lot of different functions, explained in the documentation of gensim. Please note that everytime we train a model, even with the same data, the resulting embeddings will be slightly different. This is because the neural network will use different random seeds to initialize its weights. The details of this go beyond what you will learn in this lab, but keep in mind that when you run this notebook your results might be different in the details, but the general trends should hold. For example, the similarity scores between 'woman' and 'king' might not be exactly the same, but the most similar words for 'king' will be mostly the same. \n",
    "\n",
    "After the model is built, you can save it to disk for future usage. This may be handy if for some reason the notebook is killed or gets stuck and do you do not like drinking too much tea.\n",
    "\n",
    "We use the function *wv.save_word2vec_format* to save the model. I am storing it in a subfolder \"models\". Make sure the folder exist in the path you specify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the model as a text file or as a binary file. The binary file loads faster but you could have problems porting it from machine to machines with different OS. The text file you can load in a text editor and inspect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "englishleipzig_w2v.wv.save_word2vec_format('/Users/piek/Desktop/t-ONDERWIJS/data/leipzig-corpora/models/eng_news_2005_1M-sentences.txt')\n",
    "englishleipzig_w2v.wv.save_word2vec_format('/Users/piek/Desktop/t-ONDERWIJS/data/leipzig-corpora/models/eng_news_2005_1M-sentences.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a powerful plain text editor, you can open the txt file and inspect it. You can also use the command line and type the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12339 100\n",
      "the 1.5022856 0.098328725 0.5024047 1.5309852 -0.09105321 0.25754008 1.0062594 -1.8975921 -0.2943641 1.6874166 -0.5590369 0.066697024 0.65090406 -0.5807375 0.51481295 1.2120371 0.74987215 0.45058954 0.47719938 1.4547876 1.0712636 0.93635213 0.1859579 -0.14195006 1.4503254 -1.1170188 -1.8843431 0.6140086 -0.95290875 0.1610322 0.6690898 0.5791449 -0.6998886 0.377161 -0.0649989 -0.43829533 0.79310054 0.22689182 0.18690485 1.2593929 -2.032061 -0.037656322 -0.5216925 -0.034906916 0.403945 0.82607985 -0.36427703 0.17960487 0.8871468 -0.57342553 -1.0268893 0.0016339723 -1.237817 -0.045479223 -0.5588587 -0.35184065 0.45267746 -0.6730506 0.24810201 1.0806748 0.049607046 0.16131048 -0.70704466 -0.4510508 0.45796368 0.5533735 0.3766744 -1.2691549 0.27146065 0.24675971 -0.4950922 0.9270032 -0.6861244 0.31925648 0.123968504 0.5584019 0.8135406 1.004157 0.38755992 0.4915214 -1.0176872 -1.8164865 0.6402584 1.8562845 -0.8639606 1.0418053 -0.5464921 -0.8213556 -0.6610205 0.9182424 -0.6670324 0.016770996 1.35076 -1.1304499 0.68158954 0.24447446 -1.0040869 0.3768761 0.17219083 -0.8493722\n",
      "to -0.757341 -2.1231844 2.5731418 0.49765608 1.1314222 -0.010478925 -0.23015144 -0.91638225 1.0209128 0.62880826 -0.71580786 1.0845423 -1.090475 0.8587042 0.32957548 2.2391868 1.5294064 1.6527305 0.03625467 1.8334652 -0.6947514 -0.34146118 -1.4036518 -3.0469182 0.53527945 2.2802253 -0.76977336 0.9374229 1.3498381 1.7657946 1.9850923 1.4841038 1.1547537 -0.2655303 0.49701768 -0.52416396 0.92445403 0.15662578 0.6680892 2.5395956 1.888203 -1.0546225 1.4678907 -2.3066995 -2.0527744 0.41255394 0.9221692 1.6022116 0.33374658 0.17159027 0.004374889 -0.18491156 0.5852123 2.4924688 -0.50849485 2.790184 1.0382446 0.86466384 0.5277848 1.784276:\u001b[K"
     ]
    }
   ],
   "source": [
    "%cat /Users/piek/Desktop/t-ONDERWIJS/data/leipzig-corpora/models/eng_news_2005_1M-sentences.txt | more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need to stop the previous cell manually in this notebook because the \"more\" command only shows the beginning of the file and waits for an enter to continue or ctrl-c for cancel. You stop the cell by clicking on the square next to the play symbol in the menu of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line has two numbers: the first is the size of the vocabulary and the second is the number of dimensions or the length of the vectors. Both depend on the parameters you used to build the model. The file contains a line for each word with its embedding representation. Depending on the parameters used, you may see the embeddings for the very frequent words \"the\" and \"to\" as the first lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we saved our model to disk, we can load it any time and use it. The next time you launch this notebook, you do not need to collect and preprocess the corpus and build a model from it. You can load the model directly from the location where you saved it. That's what we are going to do now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How to load a stored model:\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# You can load it either as text or as binary data. \n",
    "#The latter is more efficient but you may not be able to port it from machine to machine.\n",
    "englishleipzig_w2v = KeyedVectors.load_word2vec_format('/Users/piek/Desktop/t-ONDERWIJS/data/leipzig-corpora/models/eng_news_2005_1M-sentences.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that loading is much faster than building! Let's check some of the properties of the englishleipzig_w2v model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size = 100\n",
      "Vocabulary size = 12339\n"
     ]
    }
   ],
   "source": [
    "# Show some properties of our model. Notice these are also in the text file.\n",
    "print('Vector size =', englishleipzig_w2v.vector_size)\n",
    "print('Vocabulary size =', len(englishleipzig_w2v.key_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have limited the dimensions to '100' which is the vector size and the vocabulary is much smaller than the Wikipedia vocabulary and even smaller than WordNet. We can now use any word from the vocabulary as a key to obtain the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[ 0.6905258   1.5594054   0.5748212   1.5657576  -0.1534916  -0.40360108\n",
      "  0.3100164   1.0694386   1.2010936   0.00503622  0.77189213 -0.99546665\n",
      " -0.3815703  -0.09455451  0.42443958 -0.83974576  0.04821923 -0.44235227\n",
      " -0.3570004  -2.0702674   2.3530064   0.53084767  1.750268   -0.9308972\n",
      " -0.01893806 -1.2629433  -0.36071002 -0.84374225  1.1177106  -0.11203402\n",
      "  0.62249374 -0.10188994  0.21202749 -0.21265593  1.1802895  -0.23113143\n",
      " -0.24272048  0.3584279   0.3905339  -0.84558505  1.1705464  -1.819815\n",
      " -0.14620385 -0.7416933   0.66355807 -0.4930548  -0.30910316 -0.8348972\n",
      " -1.3058763   0.40529746 -0.44586256  0.0328793  -1.0537902  -0.6687268\n",
      " -0.2753386  -1.774672    0.67011094  0.2345644  -0.6156653  -0.18520372\n",
      " -1.2395182   1.0801073   0.40914643  0.33789864 -0.7158897   0.6823043\n",
      "  0.0744905   0.99532855 -0.62204605  0.32310545 -0.1312581   0.10557448\n",
      "  1.04885    -0.08423143 -0.05755047  0.6271566  -0.523815    0.6260166\n",
      "  0.19405621 -1.4696909  -0.9468328  -0.68338794  0.28935874  0.5054681\n",
      " -1.1737672   0.4397904   0.81284195  1.1003851  -0.8820864   0.34501302\n",
      "  0.19320524 -0.51701176 -0.87802094 -0.42660257  1.6690493   0.66781145\n",
      "  0.90580386  0.27522567 -0.5827837  -0.25842702]\n"
     ]
    }
   ],
   "source": [
    "king_vector = englishleipzig_w2v[\"king\"]\n",
    "print(len(king_vector))\n",
    "print(king_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we get a dense vector with values for all 100 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily find the similarity between two words. Similarity is measured as the cosine between the two word embeddings, and ranges between -1 and +1. The higher the cosine, the more similar two words are. As expected, the figures below show that *king* is closer to *queen* than to *coffee*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69186014\n",
      "0.09530553\n"
     ]
    }
   ],
   "source": [
    "print(englishleipzig_w2v.similarity(\"king\", \"queen\"))\n",
    "print(englishleipzig_w2v.similarity(\"king\", \"coffee\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar vein, we can find the words that are most similar to a target word. The words with the most similar embedding to *king* are all similar titles (such as *prince*  and *queen*) or they are semantically related to royalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.7474125027656555),\n",
       " ('buchanan', 0.7114241719245911),\n",
       " ('queen', 0.6918601393699646),\n",
       " ('mccartney', 0.6681753993034363),\n",
       " ('fraser', 0.6624840497970581),\n",
       " ('emperor', 0.6491478681564331),\n",
       " ('baldwin', 0.647771954536438),\n",
       " ('rainier', 0.6466833353042603),\n",
       " ('dawson', 0.6439461708068848),\n",
       " ('vincent', 0.6431512832641602)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "englishleipzig_w2v.similar_by_word(\"king\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this model was trained from the Leipzig news corpus for English, which is not that big! Companies such as Google, Amazon and Facebook train their models on many magnitudes more data. Much bigger corpora in many languages can be found at: https://commoncrawl.org\n",
    "\n",
    "Note that training a model on such data sets also requires a powerful computing infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we can look for words that are similar to a set of words and dissimilar to another set of words at the same time. This allows us to look for analogies of the type *king is to man like ... is to woman*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.726746141910553),\n",
       " ('prince', 0.6802133321762085),\n",
       " ('anne', 0.6743527054786682),\n",
       " ('caroline', 0.6428135633468628),\n",
       " ('diana', 0.6412346959114075),\n",
       " ('buchanan', 0.6354133486747742),\n",
       " ('mary', 0.6349809169769287),\n",
       " ('elizabeth', 0.632943868637085),\n",
       " ('wong', 0.6183045506477356),\n",
       " ('rainier', 0.6174156665802002)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "englishleipzig_w2v.most_similar(positive=['woman', 'king'], negative=[\"man\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that *queen* (scoring slightly higher) and *prince* (scoring significantly lower). More female names are included and male names are scoring lower or even disappeared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words are not word meanings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens with ambiguous words?\n",
    "\n",
    "If we take an ambiguous word such as **mouse**, we get devices but not similar animal terms such as 'rat'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('keyboard', 0.712760329246521),\n",
       " ('printer', 0.6471728682518005),\n",
       " ('laptop', 0.6370227932929993),\n",
       " ('blade', 0.6339210867881775),\n",
       " ('clips', 0.6317967772483826),\n",
       " ('monkey', 0.6317648887634277),\n",
       " ('robot', 0.631564736366272),\n",
       " ('bits', 0.630458414554596),\n",
       " ('lens', 0.6261634826660156),\n",
       " ('colour', 0.6257264614105225)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "englishleipzig_w2v.most_similar(positive=[\"mouse\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we however subtract the vector for **computer** or **keyboard**, we can try to get more animal related terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smell', 0.5303334593772888),\n",
       " ('lovely', 0.5213074088096619),\n",
       " ('colours', 0.5054668188095093),\n",
       " ('dark', 0.5025986433029175),\n",
       " ('roses', 0.5024785995483398),\n",
       " ('monkey', 0.500421941280365),\n",
       " ('fried', 0.4910123646259308),\n",
       " ('beats', 0.4807605445384979),\n",
       " ('hell', 0.473664790391922),\n",
       " ('potter', 0.4730132818222046)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "englishleipzig_w2v.most_similar(positive=[\"mouse\"], negative=[\"computer\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really a good result although *keyboard* is no longer on top. This may be due to the size of the corpus used and the settings that were used to build it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can present the word2vec model with a list of words and ask it to identify the odd one out. It then uses the word embeddings to identify the word that is least similar to the other ones. For example, in the list *car, bike, bus, coffee*, it correctly identifies *coffee* as the odd one out. In the list *coffee, car, tea, milk*, it correctly singles out *car*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee\n",
      "bus\n"
     ]
    }
   ],
   "source": [
    "print(englishleipzig_w2v.doesnt_match(\"car bike bus coffee\".split()))\n",
    "print(englishleipzig_w2v.doesnt_match(\"coffee bus bike tea milk\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting embeddings [Advanced]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize some of our embeddings. To plot embeddings with a dimensionality of 100 or more, we first need to map them to a dimensionality of 2. We do this with the popular [t-SNE](https://lvdmaaten.github.io/tsne/) method. T-SNE, short for t-distributed Stochastic Neighbor Embedding, helps us visualize high-dimensional data by mapping similar data to nearby points and dissimilar data to distance points in the low-dimensional space.\n",
    "\n",
    "T-SNE is included in [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). If you have not installed scikit-learn you need to do that first. Follow the instructions at: https://scikit-learn.org/stable/install.html to do so or run `conda install scikit-learn`. Note that we will use scikit learn anyway next week.\n",
    "\n",
    "To run the TSNE package within Scikit-learn, we just have to specify the number of dimensions we'd like to map the data to (`n_components`), and the similarity metric that t-SNE should use to compute the similarity between two data points (`metric`). We're going to map to 2 dimensions and use the cosine as our similarity metric. Additionally, we use PCA as an initialization method to remove some noise and speed up computation. The [Scikit-learn user guide](https://scikit-learn.org/stable/modules/manifold.html#t-sne) contains some additional tips for optimizing performance. \n",
    "\n",
    "Plotting all the embeddings in our vector space would result in a very crowded figure where the labels are hardly legible. Therefore we'll focus on a subset of embeddings by selecting the 'n' most similar words to a target word. In the example below, we set the target word to 'moon' and we draw only the graph for the 50 most related words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualise a model and its words we need an auxiliary function that is given below. Run the cell below to make this function 'tsne_plot_target_word' available to your notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also need to install matplotlib (a library for plotting) and pandas (a library for data analysis). You can do so in the usual way:\n",
    "\n",
    "`conda install matplotlib`\n",
    "\n",
    "`conda install pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The python kernel does not appear to be a conda environment.  Please use ``%pip install`` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstall matplotlib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconda\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall pandas\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2417\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2415\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2416\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2417\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2419\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2420\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2421\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/lib/python3.9/site-packages/IPython/core/magics/packaging.py:87\u001b[0m, in \u001b[0;36mPackagingMagics.conda\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run the conda package manager within the current kernel.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mUsage:\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m  %conda install [pkgs]\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_conda_environment():\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe python kernel does not appear to be a conda environment.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use ``\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mpip install`` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m conda \u001b[38;5;241m=\u001b[39m _get_conda_executable()\n\u001b[1;32m     91\u001b[0m args \u001b[38;5;241m=\u001b[39m shlex\u001b[38;5;241m.\u001b[39msplit(line)\n",
      "\u001b[0;31mValueError\u001b[0m: The python kernel does not appear to be a conda environment.  Please use ``%pip install`` instead."
     ]
    }
   ],
   "source": [
    "%conda install matplotlib\n",
    "%conda install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define an auxiliary function that given a model and a selection of similar words generates the 2-dimentional TSNE space and saves it to a file with name of the target_word and the number of neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne\n",
    "#https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.savefig.html\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def tsne_plot_target_word(model, selected_words, target_word):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for i, word in enumerate(selected_words):\n",
    "        tokens.append(model[i])\n",
    "        labels.append(word)\n",
    "\n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.title('Word embedding space:'+target_word)\n",
    "    plt.savefig(target_word+str(len(selected_words))+'_englishleipzig_w2v.pdf', dpi=600, transparent='true', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# We select a set of 50 words most similar to 'moon' and store the result as 'selected_words'\n",
    "count=50\n",
    "target_word = \"moon\"\n",
    "selected_words = [w[0] for w in englishleipzig_w2v.most_similar(positive=[target_word], topn=count)]\n",
    "print('The 50 most similar words to \"moon\" in a our model:', selected_words)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we obtain the vector representation for the top 50 words and create an array of vector arrays with scores for each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = [englishleipzig_w2v[w] for w in selected_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the first two elements. It is indeed a list with vector arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(embeddings[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we call the TSNE function with the parameters mentioned before and we call a fit_transform(embeddings) function to fit in our selected embeddings. This will apply the cosine similarity to all embeddings and plot these in two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapped_embeddings = TSNE(n_components=2, metric='cosine', init='pca').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply our custom made function 'tsne_plot_target_word' to the mapped_embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsne_plot_target_word(embeddings, selected_words,target_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks nice! So our target word 'moon' is located at the coordinates [0,0]. Inspect the diagram to see what words are close. OK-ish. Perhaps, it works better with 300 dimensions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the full graph\n",
    "\n",
    "To display the whole space we rebuild the word2vec model and limit the vocabulary to frequency of 500 or more and reduce the dimensions to 50. This may still take a while to complete. The result may look cluttered, which is why we save it to a PDF file to expand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "count=500\n",
    "\n",
    "englishleipzig_w2v = Word2Vec(text_leipzigcorpus_clean, vector_size =50, window =4, min_count =count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the result to disk, we need another customized function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne\n",
    "#https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.savefig.html\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def tsne_plot(model, name):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.index_to_key:\n",
    "        tokens.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.title('Word embedding space:'+name)\n",
    "    plt.savefig(name, dpi=600, transparent='true', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call the new customized function 'tsne_plot' to the full model. It took a while on my computer to save it. If you dont manage, you can inspect the PDF that is included in the LAB distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsne_plot(englishleipzig_w2v, 'moon_englishleipzig_w2v.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hltenv",
   "language": "python",
   "name": "hltenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
