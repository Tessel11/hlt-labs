{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 How to use GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT (Generative Pretrained Transformer) is a model trained to generate text given a preceding input (Brown et al 2020) It can do this repetitively up to a certain length, likewise generating short stories.\n",
    "\n",
    "Another generative model is T5 (Text to Text Transfer Transformer, Raffel et al. 2019). T5 models many tasks as a text generation task, ranging from plain translation, sentiment annotation, question-answering, similarity, to summarisation. Tasks are differentiated through prompt prefixes.\n",
    "\n",
    "<img src=\"T5.gif\">\n",
    "\n",
    "Models such as GPT4 and T5, although having good performance, are by far too large model to work with. Therefore in this notebook, we look into an older model GPT2, which is smaller and publicly available. It is nevertheless a generative model designed just like the others.\n",
    "\n",
    "### References\n",
    "\n",
    "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n",
    "\n",
    "OpenAI, 2023. GPT-4 Technical Report. arXiv:2303.08774\n",
    "\n",
    "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text trans- former. arXiv preprint arXiv:1910.10683.\n",
    "\n",
    "Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\" arXiv preprint arXiv:1910.01108 (2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models for English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load GPT2 from the Huggingface platform as we did before for BERT and XLM-RoBERTa as part of a pipeline. We now specify the task as **text-generation**. As the model is big, it may take a while to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)neration_config.json: 100%|███████████████████████████████████████████| 124/124 [00:00<00:00, 16.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "gpt2pipe = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you succesfully downloaded it, it is saved on disk in cache for futher use. The next time you load the model it will be faster from disk.\n",
    "\n",
    "You can now pass in any text as a prompt to this pipeline instance and it will complete the text according to the model. We create a list of prompts that are very similar except for the populair entity as the subject. In this way, we can test of the model also generates different texts relevant for the different entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = ['Boris Johnson is called to justice for',\n",
    "           'Donald Trump is called to justice for', \n",
    "           'Angela Merkel is called to justice for']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Boris Johnson is called to justice for his actions. He\\'s a man of the people. \"It would be a great honour to sit down together and speak to the families of those killed in the London Bridge terror attack,\" said Mr Johnson.\\n\\n'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Donald Trump is called to justice for the horrific attack he is accused of assaulting a woman that sparked protests Saturday in Charlottesville that included a violent confrontation that left one woman dead.\\n\\nThe president said in a Twitter post: \"My administration should do all'}]\n",
      "[{'generated_text': \"Angela Merkel is called to justice for her 'inappropriate' behaviour amid a row over her treatment of asylum seekers. The Germany Interior Ministry is looking into whether she breached a federal statute by telling local authorities to be patient with those who seek justice.\"}]\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    print(gpt2pipe(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the stories are different for each entity but also show specific details that seem relevant for each. Whether these stories are correct and factual is a diffeent thing. Generative models do not index facts but make up facts based on word probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a powerful computer, GPT2 may be to big to use or too slow. Don't worry! Researchers found a way to compress large models to smaller more efficient models with almost equal performance. Knowledge distillation is a compression technique in which a smaller model is trained to reproduce the behaviour of a larger model or an ensemble of models. The distilled model is trained with a distillation loss over the soft target probabilities of the original model (Sanh et al., 2019).\n",
    "\n",
    "There is also a distilled version of GTP2 called *distilgpt2*, which is smaller (only 40% of the original parameters) and faster while it is claimed to have almost equal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)neration_config.json: 100%|███████████████████████████████████████████| 124/124 [00:00<00:00, 65.4kB/s]\n"
     ]
    }
   ],
   "source": [
    "distilgpt2pipe = pipeline(\"text-generation\", model=\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Boris Johnson is called to justice for the millions he has made in support of the EU in light of the decision to leave the single market.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Donald Trump is called to justice for his support of the U.S.'s military in Iraq after a U.S. soldier told him they would be prosecuted for aiding and abetting the Islamic State — which the U.S. says has carried\"}]\n",
      "[{'generated_text': 'Angela Merkel is called to justice for her alleged mishandling of the refugee crisis, German media say, and on Monday called for changes to the law which enables her to seek asylum.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    print(distilgpt2pipe(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives very different output for our prompts. DistilGPT2 has substantial less parameters (40%) and apparently represents less knowledge for the targets entities. The stories are shorter and contains less entity specific details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 for other languages than English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a GPT model from scratch is costly. You not only need a lot of data but also computer power to create such a model. An interesting alternative is to only train the vocabulary part of a model for a language and to keep the hidden layers of the English model for the contextual attention relations and capability to predict the next token embeddings. You can imagine that once the words in a sentence from a language get reasonable embedding representations, similar relations will hold through the attention mechanism across these embedding representations learned from the English data.\n",
    "\n",
    "Such an apporach was followed by *de Vries and Nissim (2021)* from Groningen University for Dutch and Italian. You can read the paper for more details.\n",
    "\n",
    "References:\n",
    "\n",
    "de Vries, Wietse, and Malvina Nissim. \"As good as new. How to successfully recycle English GPT-2 to make models for other languages.\" arXiv preprint arXiv:2012.05628 (2020). https://aclanthology.org/2021.findings-acl.74.pdf\n",
    "\n",
    "See also: https://github.com/wietsedv/gpt2-recycle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the resulting GPT2 models for Dutch and Italian from Huggingface and generate a Dutch and Italian short story from a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dutchGpt2pipe = pipeline(\"text-generation\", model=\"GroNLP/gpt2-small-dutch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dutch_prompts = ['Mark Rutte is ter verantwoording geroepen voor',\n",
    "           'Thierry Baudet is ter verantwoording geroepen voor', \n",
    "           'Thierry Rutte is ter verantwoording geroepen voor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/.pyenv/versions/3.9.16/lib/python3.9/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (100) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Mark Rutte is ter verantwoording geroepen voor de mislukte coup van Erdogan in november 2016. \"Ik weet dat wij het niet goed hebben, maar ik ben er wel overheen gekomen\", twitterde hij zondag na zijn speech op NPO Radio 1.\\nHet Turkse parlement wil een nieuw kabinet vormen waarin Turkije als premier wordt gekozen. Een nieuwe coalitie zal moeten worden gevormd tussen minister-president Tayyip Erdogan (links) en vicepremier Küdim Aktasoglu (rechts). Er staat volgens sommige peilingen'}]\n",
      "[{'generated_text': 'Thierry Baudet is ter verantwoording geroepen voor de moord op Marianne Thieme aan het begin van haar politieke carrière.\\nIn een interview in Trouw vertelt Thierry Baudet: \"Ik zou me niet voorstellen dat er ooit iemand zoiets zo verschrikkelijks te maken had, maar ik heb ook nog nooit iets dergelijks meegemaakt.\" Hij verwijst naar zijn eigen ervaringen met geweld tegen vrouwen en sekswerkers die door hem werden beschuldigd van \\'onbehoorlijke of gewelddadige\\' vormen van seksuele intimidatie (de Volkskrant, 18 februari). In'}]\n",
      "[{'generated_text': 'Thierry Rutte is ter verantwoording geroepen voor zijn uitspraken over de aanslagen in Parijs. \"Ik ben van plan mijn excuses af te zetten\", zei hij donderdag tijdens een persconferentie op Twitter.\\nDefensieminister Ard van der Steur (Veiligheid en Justitie, VVD): \"Het spijt me heel erg dat ik na zo\\'n lange periode niet aan het licht kwam.\"Premier Mark Rutte had eerder deze week al gezegd dat er sprake was van \\'een terreurbeweging\\'. Hij doelde daarmee op IS-leider'}]\n"
     ]
    }
   ],
   "source": [
    "for prompt in dutch_prompts:\n",
    "    print(dutchGpt2pipe(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': \"Een klein kind, maar ik weet zeker dat ze er niet in staat is om aan te geven wat zij wil. Ze moet het zelf weten.'\\n 'Ik zal haar vertellen waar we mee bezig zijn,' zei hij met een ondeugende twinkeling in zijn ogen. De jongen knikte instemmend en ging achter zich op naar de auto die hen hadden gezet. Een paar minuten later was hun huis binnengeleid door twee andere mannen van ongeveer veertig of vijftig meter vijfenvijftig tot zestig jaar oud. Zij waren gekleed in zwarte\"}\n"
     ]
    }
   ],
   "source": [
    " print(dutchGpt2pipe('Een klein kind')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'Uno bambino picolosa non ha mai potuto fare il male, è stato preso in custodia da un uomo che gli aveva chiesto di aiutarlo a trovare i soldi per la morte del padre.\\n\\nL\\'episodio narra le vicende delle due sorelle e l\\'evoluzione degli atteggiamenti dei loro genitori nella vita quotidiana dell\\'adolescente (le storie si mescolano anche con altri episodi). Il narratore racconta così una storia narrata nei flash della prima parte dello stesso romanzo: \"Il figlio era morto nel suo letto'}\n"
     ]
    }
   ],
   "source": [
    "italianGpt2pipe = pipeline(\"text-generation\", model=\"GroNLP/gpt2-small-italian\")\n",
    "\n",
    "print(italianGpt2pipe('Uno bambino picolo')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger generative models such as ChatGPT, BARD, LLAMA have seen data in many languages (although still dominantly English). These models can generate text in these languages as well without further training. However, research into the multilinguality of these models is ongoing and the dominance of English appears to have an impact on the language generated for non-English languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
